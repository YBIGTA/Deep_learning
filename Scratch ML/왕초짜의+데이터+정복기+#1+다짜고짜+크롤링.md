
# ì™•ì´ˆì§œì˜ ë°ì´í„° ì •ë³µê¸° 

ë³¸ì¸ì€ ë°ì´í„° ì‹¸ì´ì–¸ìŠ¤ì— ì…ë¬¸í•œ ì™• ì´ˆì§œì´ë‹¤.

í†µê³„ëŠ” í‰ê· ì´ ë¬´ì—‡ì¸ì§€ ì •ë„ ì•Œê³  í”„ë¡œê·¸ë˜ë°ì€ íŒŒì´ì¬ìœ¼ë¡œ Hello! world ì¶œë ¥ì´ ê°€ëŠ¥í•œ ìˆ˜ì¤€ì´ë‹¤.

ì „ê³µì€ ê²½ì œí•™ê³¼ë©° ì·¨ì—…ì„ ëª©ì „ì— ë‘” 4í•™ë…„ í•™ìƒì´ë‹¤.

ë³¸ì—…ì¸ ê²½ì œí•™ ê³µë¶€ë¥¼ ì—´ì‹¬íˆí•´ì„œ ê¸ˆìœµê³µê¸°ì—… ì¤€ë¹„ë¥¼ í•˜ê±°ë‚˜ ê³ ì‹œë¥¼ í•˜ëŠ”ê²Œ ì˜¬ë°”ë¥´ë‹¤í•˜ëŠ” ê¸¸ì´ì§€ë§Œ í•œêµ­ì‚¬ ì‹œí—˜ë§Œ ë”°ê³  ì§„ì‘ì— ê·¸ë§Œë‘ì—ˆë‹¤.

í•˜ì§€ë§Œ ì™ ì§€ ì¸ê³µì§€ëŠ¥ì— ë§¤ë£Œë˜ì—ˆê³  ë¨¸ì‹ ëŸ¬ë‹ì— ë°˜í–ˆë‹¤.

ê·¸ë˜ì„œ ë°ì´í„° ì‹¸ì´ì–¸ìŠ¤ë¥¼ ì‹œì‘í•˜ê³ ì í•œë‹¤.

ì¼ê¸°ì‹ìœ¼ë¡œ ë‚´ê°€ ë°°ìš´ê²ƒì„ ì •ë¦¬í•˜ê³  ê¸°ë¡í•˜ëŠ” ë°©ì‹ìœ¼ë¡œ ê¸€ì„ ì´ì–´ë‚˜ê°€ê³ ì í•œë‹¤.

ë§Œì•½ ì´ ê¸€ì€ ì½ëŠ” ì´ê°€ ë‚´ ìì‹ ì´ ì•„ë‹Œ íƒ€ì¸ì´ë¼ë©´ ê·¸ëŒ€ì—ê²Œë„ í–‰ìš´ì„ ë¹Œì–´ì£¼ê³  ì‹¶ë‹¤.

ê±°ì°½í•œ ì œëª©ê³¼ëŠ” ë‹¤ë¥´ê²Œ ë°ì´í„° ì‚¬ì´ì–¸ìŠ¤ë¥¼ ì •ë³µí•˜ê¸´ í˜ë“¤ ë“¯ ì‹¶ë‹¤.

ê·¼ë° ë­ í˜¹ì‹œ ëª¨ë¥´ë‹ˆê¹Œ

# #1 ë‹¤ì§œê³ ì§œ í¬ë¡¤ë§

![image.png](attachment:image.png)

ìœ„ì˜ ì±…ì´ ë‚´ê°€ ì„ íƒí•œ ì²«ë²ˆì§¸ êµì¬ì´ë‹¤.

í•˜ì§€ë§Œ ì·¨ì—… ìŠ¤í™ì„ ìœ„í•´ ì¼ë‹¨ ë¹…ë°ì´í„° ê´€ë ¨ ê³µëª¨ì „ì„ ì‹œì‘í•´ë²„ë ¸ë‹¤.

![image.png](attachment:image.png)

ê·¸ë˜ì„œ ì´ ì±…ì˜ 

# 9ì¥ì¸ í¬ë¡¤ë§

ì„ ë¨¼ì €í•˜ê¸°ë¡œ í–ˆë‹¤

ê°œë´‰ ì˜ˆì • ì˜í™” ê´€ê°ìˆ˜ë¥¼ ì˜ˆì¸¡ì´ ì´ë²ˆ ê³µëª¨ì „ì˜ ì£¼ì œì˜€ë‹¤.

ê·¸ë˜ì„œ ëŒ€ì¶© ë‹¤ì¤‘íšŒê·€ë¡œ ëª¨ë¸ì„ ë§Œë“¤ì–´ì„œ ì˜ˆì¸¡í•˜ë ¤ê³  í–ˆë‹¤.

ê·¸ëŸ¬ëŸ¬ë©´ ì—¬ëŸ¬ê°€ì§€ ë…ë¦½ë³€ìˆ˜ë“¤ì´ í•„ìš”í•œë° ê·¸ì¤‘ í•˜ë‚˜ë¡œ íŠ¸ìœ„í„°ì—ì„œ  íŠ¹ì •ê¸°ê°„(ê°œë´‰ 30ì¼ì „)ë™ì•ˆì˜ íŠ¹ì •ë‹¨ì–´(ì˜í™”ì œëª©)ì´ ì–¸ê¸‰ëœ íŠ¸ìœ— ê°œìˆ˜ë¥¼ ì¸¡ì •í•˜ê¸°ë¡œ í–ˆë‹¤.

ì¼ë‹¨ íŠ¸ìœ„í„° APIë¥¼ ì´ìš©í•˜ì—¬ ì¸¡ì •í•˜ë ¤ ì‹œë„í–ˆë‹¤.

ì¼ë‹¨ API access í† í°ì„ ë°›ì•˜ë‹¤! ì˜¤ì˜ˆ~

https://www.youtube.com/watch?v=pUUxmvvl2FE
ì—¬ê¸°ë¥¼ ì°¸ì¡°í•˜ë©´ ì–´ë–»ê²Œ API access í† í°ì„ ì–»ì„ ìˆ˜ ìˆëŠ”ì§€ ë‚˜ì™€ìˆë‹¤.
ë¬¼ë¡  ì˜ì–´ì§€ë§Œ í™”ë©´ê·¸ëŒ€ë¡œ ë”°ë¼í•˜ë©´ ëœë‹¤.

íŠ¸ìœ„í„° APIë¥¼ ì‚¬ìš©í•˜ëŠ” ì—¬ëŸ¬ê°€ì§€ ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ìˆëŠ”ë° tweepyë¥¼ ì‚¬ìš©í–ˆë‹¤!

!pip install tweepy


```python
import tweepy
import pandas as pd
import matplotlib.pyplot as plt
```


```python
from tweepy import Stream
from tweepy import OAuthHandler
from tweepy.streaming import StreamListener

consumer_key ='aJ8sfZ6a7IC7fncSIGKy9YHp3'
consumer_secret = 'NlrLeIKPpwfKzXaJNKpNLM2dXLnxcSJWB6Xr3RYsTvqEfrBlsE'
auth = tweepy.OAuthHandler(consumer_key=consumer_key, consumer_secret=consumer_secret)
api = tweepy.API(auth)

atoken = '909765471217508354-X9qJDspMiPGGE795Y97RDF4aXdXW5uK'
asecret = '909765471217508354-X9qJDspMiPGGE795Y97RDF4aXdXW5uK'

```


```python
results = []
for tweet in tweepy.Cursor(api.search, q='ì œ 7ê¸°ì‚¬ë‹¨',since='2015-07-01', count=100).items():
    results.append(tweet)
    
print(len(results))
```

    4
    


```python
def process_results(results):
    id_list = [tweet.id for tweet in results]
    data_set = pd.DataFrame(id_list, columns=["id"])

    # Processing Tweet Data

    data_set["text"] = [tweet.text for tweet in results]
    data_set["created_at"] = [tweet.created_at for tweet in results]
    data_set["retweet_count"] = [tweet.retweet_count for tweet in results]
    data_set["favorite_count"] = [tweet.favorite_count for tweet in results]
    data_set["source"] = [tweet.source for tweet in results]

    # Processing User Data
    data_set["user_id"] = [tweet.author.id for tweet in results]
    data_set["user_screen_name"] = [tweet.author.screen_name for tweet in results]
    data_set["user_name"] = [tweet.author.name for tweet in results]
    data_set["user_created_at"] = [tweet.author.created_at for tweet in results]
    data_set["user_description"] = [tweet.author.description for tweet in results]
    data_set["user_followers_count"] = [tweet.author.followers_count for tweet in results]
    data_set["user_friends_count"] = [tweet.author.friends_count for tweet in results]
    data_set["user_location"] = [tweet.author.location for tweet in results]

    return data_set
data_set = process_results(results)
```


```python
data_set.head(10)
```




<div>
<style>
    .dataframe thead tr:only-child th {
        text-align: right;
    }

    .dataframe thead th {
        text-align: left;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>id</th>
      <th>text</th>
      <th>created_at</th>
      <th>retweet_count</th>
      <th>favorite_count</th>
      <th>source</th>
      <th>user_id</th>
      <th>user_screen_name</th>
      <th>user_name</th>
      <th>user_created_at</th>
      <th>user_description</th>
      <th>user_followers_count</th>
      <th>user_friends_count</th>
      <th>user_location</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>909102490460622848</td>
      <td>@Kimsound_ ì•„ ì„¸ìƒì— ì†Œë¦¬ë‹˜ ì œê°€ ì•„ë“œë¦¬ì•™ìŠ¤ìŠ¹ì„ ì„¤ì •í• ë•Œ í‰ë¯¼ 2ê¸°ì‚¬ë‹¨ì´ì—ˆ...</td>
      <td>2017-09-16 17:11:38</td>
      <td>0</td>
      <td>0</td>
      <td>Twitter for Android</td>
      <td>899151272296693765</td>
      <td>178X173_co</td>
      <td>ì•„ë“œë¦¬ì•™ ë’¤ ğŸŸ</td>
      <td>2017-08-20 06:09:03</td>
      <td>ì°¸ì¹˜ì–‘ ( @178X173)ì˜ ì»¤ë®¤ê³„ì…ë‹ˆë‹¤! ì»¤ë®¤ì´ì•¼ê¸°ë¥¼ í•©ë‹ˆë‹¤! ì˜¤ë„ˆë‹˜ë“¤ ì €ë‘ ë†€...</td>
      <td>29</td>
      <td>35</td>
      <td></td>
    </tr>
    <tr>
      <th>1</th>
      <td>909069578067623936</td>
      <td>RT @TurmiBear: ì œ íŠ¸ìœ„í„° ë³´ì‹œëŠ” ë¶„ë“¤ ì´ê±° ë¼ì›Œë§ì¶”ê¸° í•¨ ë°°ë³´ì£ ?\n2...</td>
      <td>2017-09-16 15:00:51</td>
      <td>166</td>
      <td>0</td>
      <td>Twitter for Android</td>
      <td>742668751959523328</td>
      <td>flesruoykcufgo</td>
      <td>[ ë°˜ë™ê²° ] íœ˜ëª¨ë¦¬</td>
      <td>2016-06-14 10:43:23</td>
      <td>1ì°¨â€¢ì¡ ê·¸ë¦¼/ì»¤ë®¤ëŸ¬ | ì•¼í–‰ì„± | fub free | ì†Œì¬ì£¼ì˜ | ê¸°ìŠµë§ˆìŒì£¼ì˜ |...</td>
      <td>271</td>
      <td>290</td>
      <td>.</td>
    </tr>
    <tr>
      <th>2</th>
      <td>908986761082150912</td>
      <td>RT @TurmiBear: ì œ íŠ¸ìœ„í„° ë³´ì‹œëŠ” ë¶„ë“¤ ì´ê±° ë¼ì›Œë§ì¶”ê¸° í•¨ ë°°ë³´ì£ ?\n2...</td>
      <td>2017-09-16 09:31:46</td>
      <td>166</td>
      <td>0</td>
      <td>Twitter Web Client</td>
      <td>2323114669</td>
      <td>TurmiBear</td>
      <td>í„°ë¯¸ë² ì–´</td>
      <td>2014-02-02 02:20:02</td>
      <td>ì¤‘ì„¸-ë¥´ë„¤ìƒìŠ¤ ë¬´ìˆ  ìˆ˜ë ¨ì, ì¹¼ë•í›„, ì „ê·¼ëŒ€ ë•í›„.  êµ¬í˜¸ê¸°ì‚¬ë‹¨ ì½”ìŠ¤ ì¤€ë¹„ì¤‘Asso...</td>
      <td>803</td>
      <td>1873</td>
      <td></td>
    </tr>
    <tr>
      <th>3</th>
      <td>908437392817758208</td>
      <td>RT @8castle: &amp;lt;ê¶ê¸ˆí•´ ì£½ê² ëŠ”ë° ì•ˆí’€ë¦° ì—ìŠ¤í‹°ë‹ˆì•™ ì„¤ì •&amp;gt;\n- ...</td>
      <td>2017-09-14 21:08:47</td>
      <td>295</td>
      <td>0</td>
      <td>Twitter for iPhone</td>
      <td>4861643125</td>
      <td>Chimeal_FF14</td>
      <td>ğŸ¼ğŸ¤›ğŸ˜±</td>
      <td>2016-02-05 00:29:12</td>
      <td>ì¹´ë²™í´-ìš°ìœ (ë¥¼ë•Œë¦¬ë©´)ì•™íŒ¡ / ë£¨ë¹„ë“œ(â™¥) / Ù©(à¹‘â›á´—â›à¹‘)Û¶</td>
      <td>184</td>
      <td>201</td>
      <td>ì—ê¸°ëŠ”ë˜ì „ì—ì‚´ì•„ì—„ë§ˆëŠ”ì§‘ì—ê°ˆê±°ì•¼ğŸ˜‘</td>
    </tr>
  </tbody>
</table>
</div>



# ì•„ì‰½ê²Œë„

ë§ˆì¼€íŒ…ì— ë„ì›€ì´ ë ë²•ë§Œí•œ ì •ë³´ë“¤ì€ í˜„ê¸ˆì„ ì£¼ê³  êµ¬í• ìˆ˜ë°–ì— ì—†ì—ˆë‹¤.

íŠ¸ìœ„í„° APIë¡œëŠ” ê±°ëŒ€í•œ íŠ¸ìœ— ë°ì´í„°ë“¤ì¤‘ì—ì„œ ëª‡ê°œì˜ ìƒ˜í”Œë§Œ ì–»ì„ ìˆ˜ ìˆëŠ” êµ¬ì¡°ì˜€ë‹¤.

# 5ë¶„ë‹¹ 180 ë²ˆì˜ ì¿¼ë¦¬ë§Œ ê°€ëŠ¥í–ˆê³  1ì£¼ì¼ì „ íŠ¸ìœ—ê¹Œì§€ë§Œ ì•¡ì„¸ìŠ¤ í•  ìˆ˜ ìˆì—ˆë‹¤. 

ë³¸ì¸ì€ 1088ê°œì˜ ì˜í™”ì˜ íŠ¹ì • ê¸°ê°„ë™ì•ˆì˜ ëª¨ë“  íŠ¸ìœ— ë°ì´í„°ê°€ í•„ìš”í–ˆì§€ë§Œ

íŠ¸ìœ„í„° APIë¥¼ ì´ìš©í•´ì„œëŠ” êµ¬í•  ìˆ˜ ì—†ëŠ” ì •ë³´ì˜€ë‹¤.

ì¸í„°ë„· ê²€ìƒ‰ì„ í•´ë³´ë‹ˆ  íŠ¸ìœ„í„° ë°ì´í„°ë² ì´ìŠ¤ ì ‘ê·¼ê¶Œí•œì„ êµ¬ë§¤í•œ íŠ¸ìœ„í„° ì¨ë“œíŒŒí‹°ë“¤ì—ê²Œ ì¼ì • ê¸ˆì•¡ì„ ì§€ë¶ˆí•˜ê³  ì–»ì„ ìˆ˜ ìˆëŠ” ì •ë³´ë“¤ì´ì—ˆë‹¤.

ë¬¼ë¡  ê°€ê²©ì€ ê¸°ë³¸ì´ ë°±ë§Œì›ë‹¨ìœ„ ì˜€ë‹¤.

ê·¸ë˜ì„œ APIë¥¼ ì´ìš©ì¹˜ ì•Šê³  ì§ì ‘ í¬ë¡¤ëŸ¬ë¥¼ ì œì‘í•˜ë ¤ê³  ì‹œë„í–ˆë‹¤.

https://beomi.github.io/2017/01/20/HowToMakeWebCrawler/

ìœ„ëŠ” ì›¹í¬ë¡¤ëŸ¬ ë§Œë“¤ê¸°ê°€ ì˜ ì„¤ëª…ë˜ì–´ìˆëŠ” ì›¹ì‚¬ì´íŠ¸!

# í•˜ì§€ë§Œ íŠ¸ìœ„í„°ëŠ”

ë™ì  ì›¹í¬ë¡¤ëŸ¬ê°€ í•„ìš”í•œ ì‹¸ì´íŠ¸ì˜€ë‹¤.

íŠ¸ìœ„í„° ì‹¸ì´íŠ¸ì— ì ‘ì†í•˜ì—¬ ê³ ê¸‰ê²€ìƒ‰ì„ í†µí•´ì„œëŠ” ê³¼ê±° íŠ¹ì •ì‹œì ì˜ íŠ¸ìœ— ì¡°íšŒê°€ ê°€ëŠ¥í–ˆì§€ë§Œ

ìŠ¤í¬ë¡¤ ë°”ë¥¼ ë‚´ë¦¬ë©´ ê·¸ì „ íŠ¸ìœ—ë“¤ì´ ê°±ì‹ ë˜ëŠ” ë°©ì‹ìœ¼ë¡œ ê²€ìƒ‰ê²°ê³¼ê°€ ì¡°íšŒë˜ì—ˆê¸°ì—

ê¸°ë³¸ì ì€ ì›¹í¬ë¡¤ëŸ¬ë¡œëŠ” ëª¨ë“  íŠ¸ìœ— ì •ë³´ë¥¼ ì–»ì–´ì˜¬ìˆ˜ê°€ ì—†ëŠ” êµ¬ì¡°ì˜€ë‹¤.


# ê³ ë¯¼í•˜ë˜ ì™€ì¤‘ Githubì—ì„œ ë§ë„ ì•ˆë˜ëŠ” ì •ë³´ë¥¼ ì–»ê²Œ ë˜ì—ˆë‹¤.


![image.png](attachment:image.png)

ëˆ„êµ°ê°€ê°€ íŠ¸ìœ„í„° í¬ë¡¤ëŸ¬ë¥¼ ë§Œë“¤ì–´ ë‘” ê²ƒì´ë‹¤.

ì‹¬ì§€ì–´ APIë¥¼ ì‚¬ìš©ì¹˜ì•Šê³  íŠ¸ìœ„í„° ê³ ê¸‰ê²€ìƒ‰ ì›¹í˜ì´ì§€ì— ì§ì ‘ ì ‘ê·¼í•˜ì—¬ ëª¨ë“  íŠ¸ìœ—ì„ ê°€ì ¸ì˜¤ëŠ” ë°©ì‹ìœ¼ë¡œ 

5ë¶„ë‹¹ 180ì¿¼ë¦¬ ì œí•œë„ ì—†ê³  ì¼ì£¼ì¼ì „ì´ì•„ë‹ˆë¼ ëª¨ë“  ê³¼ê±°ì •ë³´ë¥¼ ì ‘ê·¼í•  ìˆ˜ ìˆëŠ” ì‹ ë¹„ì˜ ì½”ë“œì˜€ë‹¤.

ì´ ì„œì–‘ì—ì„œ ì˜¨ ì€ìëŠ” https://github.com/taspinar/twitterscraper ì´ ì£¼ì†Œë¥¼ í†µí•´ ë§Œë‚ ìˆ˜ ìˆë‹¤.

ê·¸ íŠ¸ìœ„í„° ì›¹ í¬ë¡¤ëŸ¬ì˜ ìì„¸í•œ ì½”ë“œ ë‚´ìš©ì€ ì•„ë˜ì™€ ê°™ë‹¤


```python
# TwitterScraper
# Copyright 2016-2017 Ahmet Taspinar
# See LICENSE for details.
"""
Twitter Scraper tool
"""

__version__ = '0.1'
__author__ = 'Ahmet Taspinar'
__license__ = 'MIT'


from twitterscraper.query import query_tweets
from twitterscraper.tweet import Tweet
```


```python
"""
This is a command line application that allows you to scrape twitter!
"""
import collections
import json
from argparse import ArgumentParser
from datetime import datetime
from os.path import isfile
from json import dump
import logging

from twitterscraper import query_tweets
from twitterscraper.query import query_all_tweets


class JSONEncoder(json.JSONEncoder):

    def default(self, obj):
        if hasattr(obj, '__json__'):
            return obj.__json__()
        elif isinstance(obj, collections.Iterable):
            return list(obj)
        elif isinstance(obj, datetime):
            return obj.isoformat()
        elif hasattr(obj, '__getitem__') and hasattr(obj, 'keys'):
            return dict(obj)
        elif hasattr(obj, '__dict__'):
            return {member: getattr(obj, member)
                    for member in dir(obj)
                    if not member.startswith('_') and
                    not hasattr(getattr(obj, member), '__call__')}

        return json.JSONEncoder.default(self, obj)


def main():
    logging.basicConfig(format='%(levelname)s: %(message)s', level=logging.INFO)
    try:
        parser = ArgumentParser(
            description=__doc__
        )

        parser.add_argument("query", type=str, help="Advanced twitter query")
        parser.add_argument("-o", "--output", type=str, default="tweets.json",
                            help="Path to a JSON file to store the gathered "
                                 "tweets to.")
        parser.add_argument("-l", "--limit", type=int, default=None,
                            help="Number of minimum tweets to gather.")
        parser.add_argument("-a", "--all", action='store_true',
                            help="Set this flag if you want to get all tweets "
                                 "in the history of twitter. This may take a "
                                 "while but also activates parallel tweet "
                                 "gathering. The number of tweets however, "
                                 "will be capped at around 100000 per 10 "
                                 "days.")
        args = parser.parse_args()

        if isfile(args.output):
            logging.error("Output file already exists! Aborting.")
            exit(-1)

        if args.all:
            tweets = query_all_tweets(args.query)
        else:
            tweets = query_tweets(args.query, args.limit)

        with open(args.output, "w") as output:
            dump(tweets, output, cls=JSONEncoder)
    except KeyboardInterrupt:
        logging.info("Program interrupted by user. Quitting...")

```


```python
import json
import logging
import random
import sys
from datetime import timedelta, date
from multiprocessing.pool import Pool

import requests
from fake_useragent import UserAgent
from twitterscraper.tweet import Tweet


ua = UserAgent()
HEADERS_LIST = [ua.chrome, ua.google, ua['google chrome'], ua.firefox, ua.ff]

INIT_URL = "https://twitter.com/search?f=tweets&vertical=default&q={q}"
RELOAD_URL = "https://twitter.com/i/search/timeline?f=tweets&vertical=" \
             "default&include_available_features=1&include_entities=1&" \
             "reset_error_state=false&src=typd&max_position={pos}&q={q}"


def query_single_page(url, html_response=True, retry=3):
    """
    Returns tweets from the given URL.

    :param url: The URL to get the tweets from
    :param html_response: False, if the HTML is embedded in a JSON
    :param retry: Number of retries if something goes wrong.
    :return: The list of tweets, the pos argument for getting the next page.
    """
    headers = {'User-Agent': random.choice(HEADERS_LIST)}

    try:
        response = requests.get(url, headers=headers)
        if html_response:
            html = response.text
        else:
            json_resp = response.json()
            html = json_resp['items_html']

        tweets = list(Tweet.from_html(html))

        if not tweets:
            return [], None

        if not html_response:
            return tweets, json_resp['min_position']

        return tweets, "TWEET-{}-{}".format(tweets[-1].id, tweets[0].id)
    except requests.exceptions.HTTPError as e:
        logging.exception('HTTPError {} while requesting "{}"'.format(
            e, url))
    except requests.exceptions.ConnectionError as e:
        logging.exception('ConnectionError {} while requesting "{}"'.format(
            e, url))
    except requests.exceptions.Timeout as e:
        logging.exception('TimeOut {} while requesting "{}"'.format(
            e, url))
    if retry > 0:
        logging.info("Retrying...")
        return query_single_page(url, html_response, retry-1)

    logging.error("Giving up.")
    return [], None


def query_tweets_once(query, limit=None, num_tweets=0):
    """
    Queries twitter for all the tweets you want! It will load all pages it gets
    from twitter. However, twitter might out of a sudden stop serving new pages,
    in that case, use the `query_tweets` method.

    Note that this function catches the KeyboardInterrupt so it can return
    tweets on incomplete queries if the user decides to abort.

    :param query: Any advanced query you want to do! Compile it at
                  https://twitter.com/search-advanced and just copy the query!
    :param limit: Scraping will be stopped when at least ``limit`` number of
                  items are fetched.
    :param num_tweets: Number of tweets fetched outside this function.
    :return:      A list of twitterscraper.Tweet objects. You will get at least
                  ``limit`` number of items.
    """
    logging.info("Querying {}".format(query))
    query = query.replace(' ', '%20').replace("#", "%23").replace(":", "%3A")
    pos = None
    tweets = []
    try:
        while True:
            new_tweets, pos = query_single_page(
                INIT_URL.format(q=query) if pos is None
                else RELOAD_URL.format(q=query, pos=pos),
                pos is None
            )
            if len(new_tweets) == 0:
                logging.info("Got {} tweets for {}.".format(
                    len(tweets), query))
                return tweets

            logging.info("Got {} tweets ({} new).".format(
                len(tweets) + num_tweets, len(new_tweets)))

            tweets += new_tweets

            if limit is not None and len(tweets) + num_tweets >= limit:
                return tweets
    except KeyboardInterrupt:
        logging.info("Program interrupted by user. Returning tweets gathered "
                     "so far...")
    except BaseException:
        logging.exception("An unknown error occurred! Returning tweets "
                          "gathered so far.")

    return tweets


def eliminate_duplicates(iterable):
    """
    Yields all unique elements of an iterable sorted. Elements are considered
    non unique if the equality comparison to another element is true. (In those
    cases, the set conversion isn't sufficient as it uses identity comparison.)
    """
    class NoElement: pass

    prev_elem = NoElement
    for elem in sorted(iterable):
        if prev_elem is NoElement:
            prev_elem = elem
            yield elem
            continue

        if prev_elem != elem:
            prev_elem = elem
            yield elem


def query_tweets(query, limit=None):
    tweets = []
    iteration = 1

    while limit is None or len(tweets) < limit:
        logging.info("Running iteration no {}, query is {}".format(
            iteration, repr(query)))
        new_tweets = query_tweets_once(query, limit, len(tweets))
        tweets.extend(new_tweets)

        if not new_tweets:
            break

        mindate = min(map(lambda tweet: tweet.timestamp, new_tweets)).date()
        maxdate = max(map(lambda tweet: tweet.timestamp, new_tweets)).date()
        logging.info("Got tweets ranging from {} to {}".format(
            mindate.isoformat(), maxdate.isoformat()))

        # Add a day, twitter only searches until excluding that day and we dont
        # have complete results for that one yet. However, we cannot limit the
        # search to less than one day: if all results are from the same day, we
        # want to continue searching further into the past: either there are no
        # further results or twitter stopped serving them and there's nothing
        # we can do.
        if mindate != maxdate:
            mindate += timedelta(days=1)

        # Twitter will always choose the more restrictive until:
        query += ' until:' + mindate.isoformat()
        iteration += 1

    # Eliminate duplicates
    return list(eliminate_duplicates(tweets))


def query_all_tweets(query):
    """
    Queries *all* tweets in the history of twitter for the given query. This
    will run in parallel for each ~10 days.

    :param query: A twitter advanced search query.
    :return: A list of tweets.
    """
    year = 2006
    month = 3

    limits = []
    while date(year=year, month=month, day=1) < date.today():
        nextmonth = month + 1 if month < 12 else 1
        nextyear = year + 1 if nextmonth == 1 else year

        limits.append(
            (date(year=year, month=month, day=1),
             date(year=year, month=month, day=10))
        )
        limits.append(
            (date(year=year, month=month, day=10),
             date(year=year, month=month, day=20))
        )
        limits.append(
            (date(year=year, month=month, day=20),
             date(year=nextyear, month=nextmonth, day=1))
        )
        year, month = nextyear, nextmonth

    queries = ['{} since:{} until:{}'.format(query, since, until)
               for since, until in reversed(limits)]

    pool = Pool(20)
    all_tweets = []
    try:
        for new_tweets in pool.imap_unordered(query_tweets_once, queries):
            all_tweets.extend(new_tweets)
            logging.info("Got {} tweets ({} new).".format(
                len(all_tweets), len(new_tweets)))
    except KeyboardInterrupt:
        logging.info("Program interrupted by user. Returning all tweets "
                     "gathered so far.")

    return sorted(all_tweets)

```


```python
from datetime import datetime

from bs4 import BeautifulSoup
from coala_utils.decorators import generate_ordering


@generate_ordering('timestamp', 'id', 'text', 'user', 'replies', 'retweets', 'likes')
class Tweet:
    def __init__(self, user, id, timestamp, fullname, text, replies, retweets, likes):
        self.user = user
        self.id = id
        self.timestamp = timestamp
        self.fullname = fullname
        self.text = text
        self.replies = replies
        self.retweets = retweets
        self.likes = likes

    @classmethod
    def from_soup(cls, tweet):
        return cls(
            user=tweet.find('span', 'username').text[1:],
            id=tweet['data-item-id'],
            timestamp=datetime.utcfromtimestamp(
                int(tweet.find('span', '_timestamp')['data-time'])),
            fullname=tweet.find('strong', 'fullname').text,
            text=tweet.find('p', 'tweet-text').text or "",
            replies = tweet.find('div', 'ProfileTweet-action--reply').find('span', 'ProfileTweet-actionCountForPresentation').text or '0',
            retweets = tweet.find('div', 'ProfileTweet-action--retweet').find('span', 'ProfileTweet-actionCountForPresentation').text or '0',
            likes = tweet.find('div', 'ProfileTweet-action--favorite').find('span', 'ProfileTweet-actionCountForPresentation').text or '0'
        )

    @classmethod
    def from_html(cls, html):
        soup = BeautifulSoup(html, "lxml")
        tweets = soup.find_all('li', 'js-stream-item')
        if tweets:
            for tweet in tweets:
                try:
                    yield cls.from_soup(tweet)
                except AttributeError:
                    pass  # Incomplete info? Discard!

```

# ë„ì €íˆ ë‚´ ì‹¤ë ¥ìœ¼ë¡œëŠ” í‰ë‚´ë‚¼ìˆ˜ ì—†ëŠ” ê°„ê²°í•œ ì½”ë“œì˜€ë‹¤.

ì´ ì½”ë“œë¥¼ ë°”íƒ•ìœ¼ë¡œ ì‹¤ì œë¡œ ì˜í™” ë°ì´í„°ë¥¼ ìˆ˜ì§‘í–ˆë‹¤.


```python
import pandas as pd
import numpy as np
```


```python
df_movie= pd.read_csv('people.csv',encoding='cp949')
```


```python
df_movie.head(504)
```


```python
name = df_movie.ì˜í™”ëª…
date1 = df_movie.ê°œë´‰ì¼
date2 = df_movie.ê°œë´‰2ì¼í›„
date3 = df_movie.ê°œë´‰28ì¼ì „
date4 = df_movie.ê°œë´‰4ì¼ì „
date5 = df_movie.ê°œë´‰34ì¼ì „
prediction1 = df_movie.ë‹¨ì–´ì˜í™”í¬í•¨íŠ¸ìœ—ìˆ˜1
prediction2 = df_movie.ë‹¨ì–´ì˜í™”í¬í•¨ë¼ìŒìˆ˜1
prediction3 = df_movie.íŠ¸ìœ—ìˆ˜1
prediction4 = df_movie.ë¼ìŒìˆ˜1
prediction21 = df_movie.ë‹¨ì–´ì˜í™”í¬í•¨íŠ¸ìœ—ìˆ˜2
prediction22 = df_movie.ë‹¨ì–´ì˜í™”í¬í•¨ë¼ìŒìˆ˜2
prediction23 = df_movie.íŠ¸ìœ—ìˆ˜2
prediction24 = df_movie.ë¼ìŒìˆ˜2
```


```python
filename = 'people.csv'
```


```python
a = 515
filename = 'people.csv'

while (a<752):
    a = a + 1
    print(a)
    query = ""+name[a]+" since:"+str(date3[a].replace(" ","-"))+" until:"+str(date2[a].replace(" ","-"))
    counttweet = 0
    countlikes = 0
    for tweet in query_tweets_once(query):
        counttweet = counttweet + 1 + int(tweet.retweets.replace(",",""))
        countlikes = countlikes +int(tweet.likes.replace(",",""))
    prediction3[a] = counttweet
    prediction4[a] = countlikes
    query = ""+name[a]+" since:"+str(date5[a].replace(" ","-"))+" until:"+str(date4[a].replace(" ","-"))
    counttweet = 0
    countlikes = 0
    for tweet in query_tweets_once(query):
        counttweet = counttweet + 1 + int(tweet.retweets.replace(",",""))
        countlikes = countlikes +int(tweet.likes.replace(",",""))
    prediction23[a] = counttweet
    prediction24[a] = countlikes
    df_movie.to_csv(filename, index=False, encoding='cp949')
```



ê° ì˜í™”ì˜ ê°œë´‰ 4ì¼ì „~ê°œë´‰34ì¼ì „ ê°œë´‰2ì¼í›„~ê°œë´‰28ì¼ì „ ë™ì•ˆ ê·¸ íŠ¹ì • ì˜í™”ê°€ íšŒìëœ íŠ¸ìœ—ê°œìˆ˜ì™€ ê·¸ íŠ¸ìœ—ì´ ë¼ì´í¬ë¥¼ ë°›ì€ íšŸìˆ˜ë¥¼ ê³„ì‚°í•˜ì—¬ ì €ì¥í–ˆë‹¤

ë¬¼ë¡  ë°ì´í„°ë¥¼ ì–»ëŠ”ë° ì‹œê°„ì€ ê½¤ ì˜¤ë˜ ê±¸ë ¸ë‹¤. (ì§ì ‘ ì›¹ì‚¬ì´íŠ¸ì— ì ‘ì†í•´ ì •ë³´ë¥¼ ì–»ì–´ì˜¤ëŠ” ë°©ì‹ì´ì—ˆê¸°ì—..)


í•˜ì§€ë§Œ ê¸ˆì•¡ ì§€ë¶ˆì—†ì´ ì´ ëª¨ë“ ê±¸ ê³µì§œë¡œ í•  ìˆ˜ ìˆì—ˆë‹¤.

# ëŠë‚€ì ì´ í¬ë‹¤.

ì‚¬ì‹¤ ì²˜ìŒìœ¼ë¡œ ì˜¤í”ˆì†ŒìŠ¤ì˜ í˜ì„ ëŠê¼ˆë‹¤.

ë§Œì•½ ë‚´ê°€ ê¹ƒí—ˆë¸Œë¥¼ ëª°ëê³  íŒŒì´ì¬ì„ ì „í˜€ëª°ëë‹¤ë©´ ì˜ë½ì—†ì´ ìˆ˜ì‹­ë§Œì›ì„ ì§€ë¶ˆí• ìˆ˜ë°–ì— ì—†ì—ˆê² ì§€ë§Œ

ëª‡ë²ˆì˜ ê²€ìƒ‰ìœ¼ë¡œ (ì‚¬ì‹¤ í•˜ë£¨ê°€ ê¼¬ë°• ê±¸ë ¸ì§€ë§Œ) ë‚´ê°€ ì›í•˜ëŠ” ë°ì´í„°ë¥¼ ì •ë³´ì˜ ë°”ë‹¤ì—ì„œ ê±´ì ¸ ì˜¬ë¦´ ìˆ˜ ìˆì—ˆë‹¤.

# ë¬¼ë¡  ë‚´ê°€ ì§ì ‘ ì½”ë“œë¥¼ ì§œëŠ” ê²ƒë„ ì¤‘ìš”í•˜ì§€ë§Œ

ì„¸ìƒì—” ì´ë¯¸ ë„ˆë¬´ ì¢‹ì€ ì½”ë“œë“¤ì´ ë‚˜ì™€ìˆë‹¤. 

ê·¸ê±¸ ì˜ ì°¾ì•„ì„œ ì´ìš©í•˜ëŠ”ê²Œ ë” íš¨ê³¼ì ì¸ë“¯ ì‹¶ë‹¤.

ì´ë²ˆ ì£¼ëŠ” ë‹¹ë©´í•œ ê³¼ì œì¸ ê³µëª¨ì „ì´ ë„ˆë¬´ ê¸‰í•´ ì¼ë‹¨ ê³µë¶€ê°€ ì•„ë‹ˆë¼ í•´ê²°ë°©ë²•ì— ì‹œê°„ì„ ìŸì•„ ë¶€ì—ˆë‹¤.

## ë‹¤ìŒì£¼ë¶€í„´ ë°‘ë°”ë‹¥ë¶€í„° ì‹œì‘í•˜ëŠ” ë°ì´í„° ì‚¬ì´ì–¸ìŠ¤ 1ì¥ë¶€í„° ì°¨ê·¼ì°¨ê·¼ ì‹œì‘í•´ì•¼ê² ë‹¤.

# ì´ë²ˆì£¼ ë.
